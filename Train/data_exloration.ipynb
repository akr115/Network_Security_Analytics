{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 - imports\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "random_seed = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - Load Data\n",
    "\n",
    "data_dir_name = \"data/\"\n",
    "file_list = [ \n",
    "             \"Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\", \n",
    "             \"Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\",\n",
    "             \"Friday-WorkingHours-Morning.pcap_ISCX.csv\",\n",
    "             \"Monday-WorkingHours.pcap_ISCX.csv\",\n",
    "             \"Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\",\n",
    "             \"Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\",\n",
    "             \"Tuesday-WorkingHours.pcap_ISCX.csv\",\n",
    "             \"Wednesday-workingHours.pcap_ISCX.csv\"\n",
    "             ]\n",
    "\n",
    "# Combine all files into a single DataFrame\n",
    "data_frames = []\n",
    "df = pd.DataFrame()\n",
    "for file_name in file_list:\n",
    "    df = pd.read_csv(data_dir_name + file_name)\n",
    "    data_frames.append(df)\n",
    "combined_data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "\n",
    "print(\"Combined data shape:\", combined_data.shape)\n",
    "print(\"Columns in the dataset:\", combined_data.columns.tolist())\n",
    "# Display first few rows of the combined dataset\n",
    "print(combined_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - Visualize the distribution of the target variable\n",
    "\n",
    "#clean column name if needed\n",
    "combined_data.columns = combined_data.columns.str.strip()\n",
    "\n",
    "#clean label values\n",
    "combined_data['Label'] = combined_data['Label'].str.strip()\n",
    "\n",
    "#get distribution\n",
    "label_counts = combined_data['Label'].value_counts()\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=label_counts.index, y=label_counts.values)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(\"Distribution of Traffic Types in Wednesday Dataset\")\n",
    "plt.xlabel(\"Traffic Label\")\n",
    "plt.ylabel(\"Number of Records\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#shows class ratios\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4\n",
    "\n",
    "# Drop columns with all drop IPS, ports, IDs, etc.\n",
    "# to avoid data leakage\n",
    "columns_to_drop = ['Flow ID', 'Source IP', 'Destination IP', 'Timestamp', 'Fwd Header Length.1']  # adjust as needed\n",
    "\n",
    "for col in columns_to_drop:\n",
    "    if col in combined_data.columns:\n",
    "        combined_data.drop(col, axis=1, inplace=True)\n",
    "\n",
    "#drop constant columns(same value or not unique value for every row)\n",
    "nunique = combined_data.nunique()\n",
    "constant_cols = nunique[nunique <= 1].index.tolist()\n",
    "combined_data.drop(constant_cols, axis=1, inplace=True)\n",
    "\n",
    "#print remaining columns\n",
    "print(f\"Remaining columns: {combined_data.columns.tolist()}\")\n",
    "print(f\"Data shape after dropping columns: {combined_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 - Handle Missing Values\n",
    "\n",
    "# handle missing values if any\n",
    "missing_values = combined_data.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values[missing_values > 0])\n",
    "# For simplicity, we will drop rows with missing values\n",
    "combined_data.dropna(inplace=True)\n",
    "print(\"Data shape after dropping missing values:\", combined_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 - Data Split\n",
    "# Split the data into training and testing sets (80-20 split) equal amount of benign and malicious samples\n",
    "train_data, test_data = train_test_split(combined_data, test_size=0.2, stratify=combined_data['Label'], random_state=random_seed)\n",
    "\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "print(\"Testing data shape:\", test_data.shape)\n",
    "print(\"Training data label distribution:\\n\", train_data['Label'].value_counts())\n",
    "print(\"Testing data label distribution:\\n\", test_data['Label'].value_counts())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 - OCSVM Training Block (Fixed)\n",
    "\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA CLEANING AND PREPROCESSING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create binary classification: BENIGN vs ATTACK\n",
    "combined_data['Attack'] = (combined_data['Label'] != 'BENIGN').astype(int)\n",
    "\n",
    "# Separate features and target\n",
    "X = combined_data.drop(['Label', 'Attack'], axis=1)\n",
    "y = combined_data['Attack']\n",
    "\n",
    "print(f\"Initial data shape: {X.shape}\")\n",
    "\n",
    "# Step 1: Check for and handle infinite values\n",
    "print(\"\\nStep 1: Checking for infinite values...\")\n",
    "inf_mask = np.isinf(X.values)\n",
    "inf_count = np.sum(inf_mask)\n",
    "print(f\"Total infinite values found: {inf_count}\")\n",
    "\n",
    "if inf_count > 0:\n",
    "    inf_cols = X.columns[np.isinf(X.values).any(axis=0)]\n",
    "    print(f\"Columns with infinite values: {list(inf_cols)}\")\n",
    "    \n",
    "    # Replace infinite values with NaN\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    print(\"Infinite values replaced with NaN\")\n",
    "\n",
    "# Step 2: Check for extremely large values\n",
    "print(\"\\nStep 2: Checking for extremely large values...\")\n",
    "large_threshold = 1e10\n",
    "large_mask = np.abs(X.values) > large_threshold\n",
    "large_count = np.sum(large_mask)\n",
    "print(f\"Values larger than {large_threshold}: {large_count}\")\n",
    "\n",
    "if large_count > 0:\n",
    "    large_cols = X.columns[(np.abs(X.values) > large_threshold).any(axis=0)]\n",
    "    print(f\"Columns with extremely large values: {list(large_cols)}\")\n",
    "\n",
    "# Step 3: Handle NaN values more aggressively\n",
    "print(\"\\nStep 3: Handling NaN values...\")\n",
    "initial_nan_count = X.isnull().sum().sum()\n",
    "print(f\"Initial NaN values: {initial_nan_count}\")\n",
    "\n",
    "if initial_nan_count > 0:\n",
    "    print(\"NaN values per column:\")\n",
    "    nan_cols = X.isnull().sum()\n",
    "    print(nan_cols[nan_cols > 0])\n",
    "    \n",
    "    # Strategy: Multiple approaches to handle NaNs\n",
    "    print(\"\\nApplying comprehensive NaN handling...\")\n",
    "    \n",
    "    # First, try to fill with median\n",
    "    for col in X.columns:\n",
    "        if X[col].isnull().any():\n",
    "            median_val = X[col].median()\n",
    "            if not np.isnan(median_val):\n",
    "                X[col].fillna(median_val, inplace=True)\n",
    "                print(f\"  Filled NaNs in '{col}' with median: {median_val:.2f}\")\n",
    "            else:\n",
    "                # If median is also NaN, use mean\n",
    "                mean_val = X[col].mean()\n",
    "                if not np.isnan(mean_val):\n",
    "                    X[col].fillna(mean_val, inplace=True)\n",
    "                    print(f\"  Filled NaNs in '{col}' with mean: {mean_val:.2f}\")\n",
    "                else:\n",
    "                    # If both median and mean are NaN, use 0\n",
    "                    X[col].fillna(0, inplace=True)\n",
    "                    print(f\"  Filled NaNs in '{col}' with 0\")\n",
    "\n",
    "# Step 4: Remove any remaining problematic rows\n",
    "print(\"\\nStep 4: Final NaN cleanup...\")\n",
    "remaining_nans = X.isnull().sum().sum()\n",
    "print(f\"Remaining NaN values: {remaining_nans}\")\n",
    "\n",
    "if remaining_nans > 0:\n",
    "    print(\"Dropping rows with remaining NaN values...\")\n",
    "    initial_rows = len(X)\n",
    "    # Get indices where there are no NaN values\n",
    "    clean_indices = X.dropna().index\n",
    "    X = X.loc[clean_indices]\n",
    "    y = y.loc[clean_indices]\n",
    "    print(f\"Dropped {initial_rows - len(X)} rows with NaN values\")\n",
    "    print(f\"Remaining data shape: {X.shape}\")\n",
    "\n",
    "# Step 5: Cap extremely large values using percentiles\n",
    "print(\"\\nStep 5: Capping extreme values...\")\n",
    "for col in X.columns:\n",
    "    # Calculate robust percentiles\n",
    "    q01 = X[col].quantile(0.01)\n",
    "    q99 = X[col].quantile(0.99)\n",
    "    \n",
    "    # Only cap if there are extreme values\n",
    "    if q99 > 1e8 or q01 < -1e8:\n",
    "        print(f\"Capping extreme values in '{col}': [{q01:.2e}, {q99:.2e}]\")\n",
    "        X[col] = X[col].clip(lower=q01, upper=q99)\n",
    "\n",
    "# Step 6: Final validation\n",
    "print(\"\\nStep 6: Final data validation...\")\n",
    "print(f\"Final data shape: {X.shape}\")\n",
    "print(f\"Any infinite values: {np.isinf(X.values).any()}\")\n",
    "print(f\"Any NaN values: {X.isnull().sum().sum()}\")\n",
    "print(f\"Data range: [{X.values.min():.2e}, {X.values.max():.2e}]\")\n",
    "\n",
    "# Ensure we have valid data\n",
    "assert not np.isinf(X.values).any(), \"Still have infinite values!\"\n",
    "assert not X.isnull().any().any(), \"Still have NaN values!\"\n",
    "print(\"‚úì Data validation passed!\")\n",
    "\n",
    "# Split data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA SPLITTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Training set class distribution:\\n{pd.Series(y_train).value_counts()}\")\n",
    "\n",
    "# Extract benign samples for OCSVM training\n",
    "X_train_benign = X_train[y_train == 0].copy()\n",
    "print(f\"OCSVM training data (benign only) shape: {X_train_benign.shape}\")\n",
    "\n",
    "# Feature Scaling\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE SCALING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use StandardScaler since we've cleaned the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print(\"Applying StandardScaler...\")\n",
    "try:\n",
    "    X_train_benign_scaled = scaler.fit_transform(X_train_benign)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(\"‚úì Feature scaling completed successfully!\")\n",
    "    print(f\"Scaled training data shape: {X_train_benign_scaled.shape}\")\n",
    "    print(f\"Scaled data range: [{X_train_benign_scaled.min():.2f}, {X_train_benign_scaled.max():.2f}]\")\n",
    "    print(f\"Scaled data mean: {X_train_benign_scaled.mean():.2f}\")\n",
    "    print(f\"Scaled data std: {X_train_benign_scaled.std():.2f}\")\n",
    "    \n",
    "    # Final check for problematic values in scaled data\n",
    "    if np.isinf(X_train_benign_scaled).any() or np.isnan(X_train_benign_scaled).any():\n",
    "        raise ValueError(\"Scaled data contains inf or NaN values\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"StandardScaler failed: {e}\")\n",
    "    print(\"Trying RobustScaler...\")\n",
    "    \n",
    "    scaler = RobustScaler()\n",
    "    X_train_benign_scaled = scaler.fit_transform(X_train_benign)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print(\"‚úì RobustScaler applied successfully!\")\n",
    "\n",
    "# OCSVM Training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ONE-CLASS SVM MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Corrected OCSVM parameters (removed random_state)\n",
    "ocsvm_params = {\n",
    "    'kernel': 'rbf',\n",
    "    'gamma': 'scale',\n",
    "    'nu': 0.05,           # Fraction of training errors\n",
    "    'shrinking': True,\n",
    "    'cache_size': 1000,   # Increased cache for better performance\n",
    "    'verbose': False,\n",
    "    'max_iter': -1        # No limit on iterations\n",
    "}\n",
    "\n",
    "print(\"OCSVM Parameters:\")\n",
    "for param, value in ocsvm_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nInitializing One-Class SVM...\")\n",
    "ocsvm_model = OneClassSVM(**ocsvm_params)\n",
    "\n",
    "print(f\"Training on {X_train_benign_scaled.shape[0]:,} benign samples...\")\n",
    "print(f\"Number of features: {X_train_benign_scaled.shape[1]}\")\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    print(\"Training in progress...\")\n",
    "    ocsvm_model.fit(X_train_benign_scaled)\n",
    "    \n",
    "    print(\"‚úì OCSVM training completed successfully!\")\n",
    "    print(f\"Number of support vectors: {ocsvm_model.n_support_[0]:,}\")\n",
    "    print(f\"Support vector ratio: {ocsvm_model.n_support_[0] / len(X_train_benign_scaled):.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Training failed: {e}\")\n",
    "    print(\"Trying with smaller sample size...\")\n",
    "    \n",
    "    # Use a smaller sample if training fails\n",
    "    sample_size = min(100000, len(X_train_benign_scaled))\n",
    "    np.random.seed(42)\n",
    "    sample_indices = np.random.choice(len(X_train_benign_scaled), sample_size, replace=False)\n",
    "    X_sample = X_train_benign_scaled[sample_indices]\n",
    "    \n",
    "    print(f\"Training on reduced sample: {sample_size:,} samples\")\n",
    "    ocsvm_model.fit(X_sample)\n",
    "    print(\"‚úì Training completed on reduced dataset!\")\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    print(\"Making predictions on test set...\")\n",
    "    y_pred_ocsvm = ocsvm_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Convert OCSVM predictions: 1 (inlier/normal) -> 0 (benign), -1 (outlier/anomaly) -> 1 (attack)\n",
    "    y_pred_binary = np.where(y_pred_ocsvm == 1, 0, 1)\n",
    "    \n",
    "    print(\"Calculating performance metrics...\")\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "    precision = precision_score(y_test, y_pred_binary, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred_binary, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_binary, zero_division=0)\n",
    "    \n",
    "    print(f\"\\nüìä OCSVM Performance Metrics:\")\n",
    "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f} (Attack detection precision)\")\n",
    "    print(f\"Recall:    {recall:.4f} (Attack detection rate)\")\n",
    "    print(f\"F1-Score:  {f1:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(f\"\\nüìã Detailed Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_binary, \n",
    "                              target_names=['Benign', 'Attack'], \n",
    "                              zero_division=0))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred_binary)\n",
    "    print(f\"\\nüéØ Confusion Matrix:\")\n",
    "    print(\"           Predicted\")\n",
    "    print(\"         Benign  Attack\")\n",
    "    print(f\"Actual Benign   {cm[0,0]:6d}  {cm[0,1]:6d}\")\n",
    "    print(f\"       Attack   {cm[1,0]:6d}  {cm[1,1]:6d}\")\n",
    "    \n",
    "    # Additional metrics\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìà Additional Metrics:\")\n",
    "    print(f\"True Positives (Attacks detected):     {tp:,}\")\n",
    "    print(f\"True Negatives (Benign correctly):    {tn:,}\")\n",
    "    print(f\"False Positives (False alarms):       {fp:,}\")\n",
    "    print(f\"False Negatives (Missed attacks):     {fn:,}\")\n",
    "    print(f\"Specificity (True negative rate):     {specificity:.4f}\")\n",
    "    print(f\"False Positive Rate:                  {fpr:.4f}\")\n",
    "    print(f\"False Negative Rate:                  {fnr:.4f}\")\n",
    "    \n",
    "    # Decision scores\n",
    "    decision_scores = ocsvm_model.decision_function(X_test_scaled)\n",
    "    print(f\"\\nüé≤ Decision Scores Statistics:\")\n",
    "    print(f\"Range:  [{np.min(decision_scores):.4f}, {np.max(decision_scores):.4f}]\")\n",
    "    print(f\"Mean:   {np.mean(decision_scores):.4f}\")\n",
    "    print(f\"Median: {np.median(decision_scores):.4f}\")\n",
    "    print(f\"Std:    {np.std(decision_scores):.4f}\")\n",
    "    \n",
    "    # Prediction distribution\n",
    "    pred_counts = pd.Series(y_pred_binary).value_counts().sort_index()\n",
    "    print(f\"\\nüìä Prediction Distribution:\")\n",
    "    print(f\"Predicted Benign:  {pred_counts.get(0, 0):,}\")\n",
    "    print(f\"Predicted Attack:  {pred_counts.get(1, 0):,}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Evaluation failed: {e}\")\n",
    "    print(\"Model was trained but evaluation encountered an error.\")\n",
    "\n",
    "# Save the model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING MODEL AND SCALER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    import pickle\n",
    "    \n",
    "    # Save model\n",
    "    model_filename = 'ocsvm_intrusion_model.pkl'\n",
    "    with open(model_filename, 'wb') as f:\n",
    "        pickle.dump(ocsvm_model, f)\n",
    "    print(f\"‚úì Model saved to: {model_filename}\")\n",
    "    \n",
    "    # Save scaler\n",
    "    scaler_filename = 'ocsvm_feature_scaler.pkl'\n",
    "    with open(scaler_filename, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(f\"‚úì Scaler saved to: {scaler_filename}\")\n",
    "    \n",
    "    # Save feature names for future reference\n",
    "    feature_names_file = 'feature_names.pkl'\n",
    "    with open(feature_names_file, 'wb') as f:\n",
    "        pickle.dump(list(X.columns), f)\n",
    "    print(f\"‚úì Feature names saved to: {feature_names_file}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving files: {e}\")\n",
    "\n",
    "print(f\"\\nüéâ OCSVM Training Pipeline Completed Successfully!\")\n",
    "print(f\"üìù Summary:\")\n",
    "print(f\"   ‚Ä¢ Original data: {combined_data.shape[0]:,} samples\")\n",
    "print(f\"   ‚Ä¢ Clean data: {X.shape[0]:,} samples\")\n",
    "print(f\"   ‚Ä¢ Training samples (benign): {X_train_benign.shape[0]:,}\")\n",
    "print(f\"   ‚Ä¢ Test samples: {X_test.shape[0]:,}\")\n",
    "print(f\"   ‚Ä¢ Features: {X.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Model: One-Class SVM (RBF kernel)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
